{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Selecting the number of components in PARAFAC models\n\nIn this example, we will look at some methods for selecting the number of components for a PARAFAC model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and utilities\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom tensorly.decomposition import parafac\n\nimport tlviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To fit PARAFAC models, we need to solve a non-convex optimization problem, possibly with local minima. It is\ntherefore useful to fit several models with the same number of components using many different random\ninitialisations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fit_many_parafac(X, num_components, num_inits=5):\n    return [\n        parafac(\n            X,\n            num_components,\n            n_iter_max=1000,\n            tol=1e-8,\n            init=\"random\",\n            orthogonalise=True,\n            linesearch=True,\n            random_state=i,\n        )\n        for i in range(num_inits)\n    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the data\nHere we load the [Aminoacids dataset](http://models.life.ku.dk/Amino_Acid_fluo) from :cite:p:`bro1997parafac`\nand plot the EEM-matrix for each of the five samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aminoacids = tlviz.data.load_aminoacids()\n\nfig, axes = plt.subplots(1, 5, figsize=(15, 3), tight_layout=True)\nfor i, sample in enumerate(aminoacids):\n    sample.plot(ax=axes[i], vmin=0, vmax=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models = {}\nfor rank in [1, 2, 3, 4, 5]:\n    print(f\"{rank} components\")\n    models[rank] = fit_many_parafac(aminoacids.data, rank, num_inits=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sort the initialisation by their SSE\n\nFor each rank, we pick the initialization run that achieved the lowest reconstruction error.\nTo do this, we first sort each initialization run by its final relative sum squared error (rel. SSE).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "errors = {}\nfor rank, inits in models.items():\n    sorted_inits, sorted_errors = tlviz.multimodel_evaluation.sort_models_by_error(inits, aminoacids.data)\n    models[rank] = sorted_inits\n    errors[rank] = sorted_errors\n\nselected_models = {rank: inits[0] for rank, inits in models.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examine model uniqueness\n\nTo examine whether we have found the global minimum, we compare each initialization run with the initialization\nrun that achieved the lowest rel. SSE. Ideally, we want the initialization runs to have reached the same point,\nand if that is the case, they should have the same rel. SSE and similar components. To measure the component\nsimilarity, we use the factor match score (FMS), similar to the cosine similarity score. An FMS value of 1 indicates\nthat the components are equivalent. The FMS is given by\n\n\\begin{align}\\text{FMS} = \\frac{1}{R}\n   \\sum_{r=1}^R\n      \\left(1 - \\frac{|w_r - \\hat{w}_r|}{\\max(w_r, \\hat{w}_r)}\\right)\n      \\frac{\\mathbf{a}_r^\\mathsf{T}\\hat{\\mathbf{a}}_r}{\\|\\mathbf{a}_r\\|\\|\\hat{\\mathbf{a}}_r\\|}\n      \\frac{\\mathbf{b}_r^\\mathsf{T}\\hat{\\mathbf{b}}_r}{\\|\\mathbf{b}_r\\|\\|\\hat{\\mathbf{b}}_r\\|}\n      \\frac{\\mathbf{c}_r^\\mathsf{T}\\hat{\\mathbf{c}}_r}{\\|\\mathbf{c}_r\\|\\|\\hat{\\mathbf{c}}_r\\|},\\end{align}\n\nwhere the the parameters without a hat correspond the the parameters of the reference decomposition.\n$w$ represents the weight of the decompositions, and $\\mathbf{a}_r, \\mathbf{b}_r$ and\n$\\mathbf{c}_r$ represents the $r$-th component vectors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fms_with_selected = {}\nfor rank, inits in models.items():\n    fms_with_selected[rank] = tlviz.multimodel_evaluation.similarity_evaluation(inits[0], inits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot uniqueness information\n\nA visual way to examine the uniqueness is to plot the rel. SSE on one axis and the FMS with selected initialization\non the other axis for each initialization run. We create one such plot for each rank to compare different choices of\nrank\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3), tight_layout=True, sharex=True, sharey=True)\n\nfor i, rank in enumerate(errors):\n    axes[i].scatter(fms_with_selected[rank], errors[rank], alpha=0.8)\n    axes[i].set_title(f\"{rank} components\")\n    axes[i].set_xlabel(\"FMS with best\")\n    axes[i].set_yscale(\"log\")\n\naxes[0].set_ylabel(\"Relative SSE\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this plot, we see that for 1-3 components, all initialization runs seem to achieve the same rel. SSE and\nsimilar components. This similarity indicates that the models are unique. However, for 4-5 components, we see\nthat the \"FMS with best\" value is low, which means that the selected initialization run is quite different from\nthe rest. This difference indicates that we either have a non-unique model or problems with local minima. Therefore,\nif we decide to go with the four or five component models, we should run even more initializations to ensure that\nwe can get the same components for more than just one initialization.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scree plot of fit and core consistency\nAnother common strategy for determining the number of components is the fit and core consistency diagnostic. The fit\nmeasures how well the model describes the data. By plotting this value for each rank choice, we can see how much\nadditional components improve the model fit. We are looking for a \"shoulder\" where the fit increase slows down,\nindicating that the added complexity of a higher rank model does not actually add much in terms of modelling\nthe data.\n\nThe core consistency measure \"interaction\" between the components of the model. The PARAFAC model assumes\nmulti-linearity, which means that the components don't interact, and the uniqueness of PARAFAC stems from this\nassumption. A low core consistency value means that allowing for component interaction could improve the model's fit,\nwhich indicates that the components are modelling behavior that is not multilinear. Therefore, the core consistency\ncan be a good metric for selecting the number of components as a drop in core consistency can indicate that you have\nadded too many components and started modelling noise or patterns that do not satisfy the multilinearity assumption\nof PARAFAC. However, the PARAFAC model can be useful even if the data does not follow the multilinearity assumption.\nHence, a low core consistency alone is not necessarily a bad sign.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n\ntlviz.visualisation.scree_plot(selected_models, aminoacids.data, metric=\"Fit\", ax=axes[0])\ntlviz.visualisation.scree_plot(selected_models, aminoacids.data, metric=\"Core consistency\", ax=axes[1])\n\naxes[1].set_ylim(0, 105)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core element plot\n\nThe core element plot shows the elements of the core tensor. Ideally \"Superdiagonal\" entries should be 1 and\noff-diagonal entries should be zero which means no interaction.\n\nOne thing to note is that as the number of components increases, so does the number of possible interactions.\nSo models with a high number of components are more likely to be improved by allowing interactions than models\nwith a low number of components. Thus, the core consistency is a less precise metric if you expect your data\nneeds a high number of components.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3), tight_layout=True, sharex=False, sharey=False)\n\nfor i, (rank, model) in enumerate(selected_models.items()):\n    axes[i].set_title(f\"{rank} components\")\n    tlviz.visualisation.core_element_plot(model, aminoacids.data, ax=axes[i])\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more information about the core consistency and core element plot, see the\n`core consistency example <core-consistency>`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split-half analysis\n\nAnother way to select the number of components is with *split-half analysis*. With split-half analysis, we\ndivide the dataset in to along one mode, and fit two different models, one for each split. Then, we compare\nthe similarity of the decomposition for the modes where we did not perform the split.\n\nFor split-half analysis, it is important to choose a split that makes sense. We need to expect that all\ncomponents will be present in both splits! In this case, we only have five samples, and reducing the number\nof samples even further can make it difficult to find the correct decomposition. We will therefore not use\nsplit-half analysis here. Instead, we have devoted a `separate example for split-half analysis <split-half>`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Component plots\n\nWhen deciding the number of components, the most important consideration is the components themselves.\nIt is therefore essential to visualize the components and evaluate whether they are meaningful in terms\nof the application\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3 components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_3comp = tlviz.postprocessing.postprocess(selected_models[3], dataset=aminoacids)\ntlviz.visualisation.components_plot(model_3comp)\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(3 * 1.6, 3), tight_layout=True)\ntlviz.visualisation.percentage_variation_plot(model_3comp, aminoacids.data, method=\"both\", ax=ax)\nax.set_yscale(\"log\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4 components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_4comp = tlviz.postprocessing.postprocess(selected_models[4], dataset=aminoacids)\ntlviz.visualisation.components_plot(model_4comp)\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(3 * 1.6, 3), tight_layout=True)\ntlviz.visualisation.percentage_variation_plot(model_4comp, aminoacids.data, method=\"both\", ax=ax)\nax.set_yscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the four-component model consists of three clear chemical spectra, which coincides well with our\nknowledge about the data. The data is samples from mixtures of three different aminoacids. The sample-mode\ncomponent shows the concentration of each chemicaland the emission- and excitation-mode components show the\nemission- and excitation-spectra of the chemicals (all in arbitrary units). With more than three components,\nwe find that one of the components have negative, and therefore unphysical, components.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}